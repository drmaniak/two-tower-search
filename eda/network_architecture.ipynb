{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramFoo(torch.nn.Module):\n",
    "  def __init__(self, voc: int, emb: int, _):\n",
    "    super().__init__()\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "    self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self, inpt, trgs, rand):\n",
    "    emb = self.emb(inpt)\n",
    "    ctx = self.ffw.weight[trgs]\n",
    "    rnd = self.ffw.weight[rand]\n",
    "    out = torch.bmm(ctx, emb.unsqueeze(-1)).squeeze()\n",
    "    rnd = torch.bmm(rnd, emb.unsqueeze(-1)).squeeze()\n",
    "    out = self.sig(out)\n",
    "    rnd = self.sig(rnd)\n",
    "    pst = -out.log().mean()\n",
    "    ngt = -(1 - rnd + 10**(-3)).log().mean()\n",
    "    return pst + ngt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = (len(words_to_ids), 64, 2)\n",
    "mFoo = SkipGramFoo(*args)\n",
    "print('mFoo', sum(p.numel() for p in mFoo.parameters()))\n",
    "opFoo = torch.optim.Adam(mFoo.parameters(), lr=0.003)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "mFoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = list(more_itertools.windowed(tokens, 3))\n",
    "inputs = [w[1] for w in windows]\n",
    "targets = [[w[0], w[2]] for w in windows]\n",
    "input_tensor = torch.LongTensor(inputs)\n",
    "target_tensor = torch.LongTensor(targets)\n",
    "dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='mlx6-word2vec', name='mFoo')\n",
    "mFoo.to(device)\n",
    "for epoch in range(2):\n",
    "  prgs = tqdm.tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "  for inpt, trgs in prgs:\n",
    "    inpt, trgs = inpt.to(device), trgs.to(device)\n",
    "    rand = torch.randint(0, len(words_to_ids), (inpt.size(0), 2)).to(device)\n",
    "    opFoo.zero_grad()\n",
    "    loss = mFoo(inpt, trgs, rand)\n",
    "    loss.backward()\n",
    "    opFoo.step()\n",
    "    wandb.log({'loss': loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositiveContextTower(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, lstm_hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        :param vocab_size: Size of the vocabulary.\n",
    "        :param embed_dim: Dimensionality of token embeddings.\n",
    "        :param lstm_hidden_dim: Number of hidden units in the LSTM.\n",
    "        :param output_dim: Final embedding dimension (common space).\n",
    "        \"\"\"\n",
    "        super(PositiveContextTower, self).__init__()\n",
    "        # Maps token indices to embeddings (each of size embed_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # LSTM that processes sequences of embeddings.\n",
    "        # With batch_first=True, input shape: (batch_size, sequence_length, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, batch_first=True)\n",
    "        # Final projection to the common embedding space\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Tensor of shape (batch_size, sequence_length) containing token indices.\n",
    "        :return: Tensor of shape (batch_size, output_dim) representing the sentence embedding.\n",
    "        \"\"\"\n",
    "        # Convert token indices to embeddings.\n",
    "        # Output shape: (batch_size, sequence_length, embed_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # The LSTM processes the sequence in order (along the sequence_length dimension)\n",
    "        # 'output' has shape (batch_size, sequence_length, lstm_hidden_dim)\n",
    "        # 'hn' (hidden state) has shape (num_layers, batch_size, lstm_hidden_dim)\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # Use the final hidden state from the last LSTM layer.\n",
    "        # This represents the entire sentence after processing all tokens.\n",
    "        last_hidden = hn[-1]  # Shape: (batch_size, lstm_hidden_dim)\n",
    "        \n",
    "        # Project the LSTM output into the common embedding space.\n",
    "        return self.fc(last_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryTower(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, lstm_hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        :param vocab_size: Size of the vocabulary.\n",
    "        :param embed_dim: Dimensionality of token embeddings.\n",
    "        :param lstm_hidden_dim: Number of hidden units in the LSTM.\n",
    "        :param output_dim: Final embedding dimension (common space).\n",
    "        \"\"\"\n",
    "        super(QueryTower, self).__init__()\n",
    "        # Maps token indices to embeddings (each of size embed_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # LSTM that processes sequences of embeddings.\n",
    "        # With batch_first=True, input shape: (batch_size, sequence_length, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, batch_first=True)\n",
    "        # Final projection to the common embedding space\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Tensor of shape (batch_size, sequence_length) containing token indices.\n",
    "        :return: Tensor of shape (batch_size, output_dim) representing the sentence embedding.\n",
    "        \"\"\"\n",
    "        # Convert token indices to embeddings.\n",
    "        # Output shape: (batch_size, sequence_length, embed_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # The LSTM processes the sequence in order (along the sequence_length dimension)\n",
    "        # 'output' has shape (batch_size, sequence_length, lstm_hidden_dim)\n",
    "        # 'hn' (hidden state) has shape (num_layers, batch_size, lstm_hidden_dim)\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # Use the final hidden state from the last LSTM layer.\n",
    "        # This represents the entire sentence after processing all tokens.\n",
    "        last_hidden = hn[-1]  # Shape: (batch_size, lstm_hidden_dim)\n",
    "        \n",
    "        # Project the LSTM output into the common embedding space.\n",
    "        return self.fc(last_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data for the query tower work on the query triplets\n",
    "import json\n",
    "with open('/Users/yuliagoryachev/Documents/mlx/mlx_week2/two-tower-search/data/train_triples_sample.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': ')what was the immediate impact of the success of the manhattan project?',\n",
       "  'positive': 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.',\n",
       "  'positive_url': 'http://www.pitt.edu/~sdb14/atombomb.html',\n",
       "  'negative': 'Parents and caregivers, can make the personal choice to reduce exposures of their infants and children to BPA: 1  Donâ€™t microwave polycarbonate plastic food containers. 2  Plastic containers have recycle codes on the bottom. 3  Reduce your use of canned foods.',\n",
       "  'negative_url': 'https://www.niehs.nih.gov/health/topics/agents/sya-bpa/index.cfm'},\n",
       " {'query': ')what was the immediate impact of the success of the manhattan project?',\n",
       "  'positive': 'The Manhattan Project and its atomic bomb helped bring an end to World War II. Its legacy of peaceful uses of atomic energy continues to have an impact on history and science.',\n",
       "  'positive_url': 'http://www.osti.gov/accomplishments/manhattan_story.html',\n",
       "  'negative': 'Do you know the meaning of Doria?@nameslook If you know the meaning of Doria, Let share on this page. Also check out the pronunciation of Doria, Doria origin, astrology, numerology and more. nameslook.com',\n",
       "  'negative_url': 'https://www.nameslook.com/doria/'},\n",
       " {'query': ')what was the immediate impact of the success of the manhattan project?',\n",
       "  'positive': 'Essay on The Manhattan Project - The Manhattan Project The Manhattan Project was to see if making an atomic bomb possible. The success of this project would forever change the world forever making it known that something this powerful can be manmade.',\n",
       "  'positive_url': 'http://www.123helpme.com/impact-of-the-manhattan-project-preview.asp?id=177337',\n",
       "  'negative': 'These body vibrations will occur through a process of induction as the Earthâ€™s force couples with the body. One way to balance and move this energy through the body is with physical exercise (i.e. walking at least several miles a day).',\n",
       "  'negative_url': 'http://in5d.com/ascension-symptoms-body-vibrations/'},\n",
       " {'query': ')what was the immediate impact of the success of the manhattan project?',\n",
       "  'positive': 'The Manhattan Project was the name for a project conducted during World War II, to develop the first atomic bomb. It refers specifically to the period of the project from 194 â€¦ 2-1946 under the control of the U.S. Army Corps of Engineers, under the administration of General Leslie R. Groves.',\n",
       "  'positive_url': 'http://www.answers.com/Q/How_did_the_Manhattan_Project_impact_on_society',\n",
       "  'negative': \"Ardex has been manufacturing car wax and related products for over half a century. Find out what tens of thousands of successful detailers in the U.S. and abroad have already discovered-that Ardex manufactures true excellence.Ardex is ...the better solution!his page uses frames, but your browser doesn't support them. Ardex has been manufacturing car wax and related products for over half a century. Find out what tens of thousands of successful detailers in the U.S. and abroad have already discovered-that Ardex manufactures true excellence. Ardex is ...\",\n",
       "  'negative_url': 'http://ardexwax.com/Product_Page.html'},\n",
       " {'query': ')what was the immediate impact of the success of the manhattan project?',\n",
       "  'positive': 'versions of each volume as well as complementary websites. The first websiteâ€“The Manhattan Project: An Interactive Historyâ€“is available on the Office of History and Heritage Resources website, http://www.cfo. doe.gov/me70/history. The Office of History and Heritage Resources and the National Nuclear Security',\n",
       "  'positive_url': 'https://www.osti.gov/manhattan-project-history/publications/Manhattan_Project_2010.pdf',\n",
       "  'negative': 'The most common cause of myocarditis is infection of the heart muscle by a virus. The virus invades the heart muscle to cause local inflammation.',\n",
       "  'negative_url': 'http://www.medicinenet.com/myocarditis/article.htm'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "42.8\n"
     ]
    }
   ],
   "source": [
    "average_sentence_length = sum(len(item['query'].split()) for item in data) / len(data)\n",
    "print(average_sentence_length)#this before stemming and removing stopwords\n",
    "average_context_length = sum(len(item['positive'].split()) for item in data) / len(data)\n",
    "print(average_context_length)#this before stemming and removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuliagoryachev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENT_LEN = 12\n",
    "CONTEXT_LEN = 50\n",
    "\n",
    "def pad_truncate(data, my_key, max_len):\n",
    "    res = []\n",
    "    for d in tqdm(data):\n",
    "        clean_sent = tokenize(d[my_key], {})\n",
    "        tokens = clean_sent[:max_len]\n",
    "        tokens += ['<unk>'] * (max_len - len(tokens))\n",
    "        res.append(tokens)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 9637.65it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 5457.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 10205.12it/s]\n"
     ]
    }
   ],
   "source": [
    "padded_query = pad_truncate(data, 'query', SENT_LEN)\n",
    "padded_positive = pad_truncate(data, 'positive', CONTEXT_LEN)\n",
    "padded_negative = pad_truncate(data, 'negative', CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['immediate',\n",
       "  'impact',\n",
       "  'success',\n",
       "  'manhattan',\n",
       "  'project',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>'],\n",
       " ['immediate',\n",
       "  'impact',\n",
       "  'success',\n",
       "  'manhattan',\n",
       "  'project',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>'],\n",
       " ['immediate',\n",
       "  'impact',\n",
       "  'success',\n",
       "  'manhattan',\n",
       "  'project',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>'],\n",
       " ['immediate',\n",
       "  'impact',\n",
       "  'success',\n",
       "  'manhattan',\n",
       "  'project',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>'],\n",
       " ['immediate',\n",
       "  'impact',\n",
       "  'success',\n",
       "  'manhattan',\n",
       "  'project',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to indices using the word_to_id dictionary\n",
    "import joblib\n",
    "word_to_id = joblib.load('/Users/yuliagoryachev/Documents/mlx/mlx_week2/two-tower-search/eda/word_to_ids.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to indices\n",
    "def convert_to_indices(data, word_to_id):\n",
    "    res = []\n",
    "    for d in tqdm(data):\n",
    "        res.append([word_to_id.get(w, word_to_id['<unk>']) for w in d])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12256.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 2238.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 2954.98it/s]\n"
     ]
    }
   ],
   "source": [
    "padded_query = convert_to_indices(padded_query, word_to_id)\n",
    "padded_positive = convert_to_indices(padded_positive, word_to_id)\n",
    "padded_negative = convert_to_indices(padded_negative, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[305701, 636225, 277834, 180836, 360087, 0, 0, 0, 0, 0, 0, 0],\n",
       " [305701, 636225, 277834, 180836, 360087, 0, 0, 0, 0, 0, 0, 0],\n",
       " [305701, 636225, 277834, 180836, 360087, 0, 0, 0, 0, 0, 0, 0],\n",
       " [305701, 636225, 277834, 180836, 360087, 0, 0, 0, 0, 0, 0, 0],\n",
       " [305701, 636225, 277834, 180836, 360087, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[817869,\n",
       "  627498,\n",
       "  1033720,\n",
       "  619830,\n",
       "  880959,\n",
       "  775439,\n",
       "  1042134,\n",
       "  277834,\n",
       "  180836,\n",
       "  360087,\n",
       "  619830,\n",
       "  1078582,\n",
       "  579722,\n",
       "  563833,\n",
       "  1011756,\n",
       "  95939,\n",
       "  737175,\n",
       "  435927,\n",
       "  760362,\n",
       "  277834,\n",
       "  667363,\n",
       "  187746,\n",
       "  531349,\n",
       "  434897,\n",
       "  537731,\n",
       "  526049,\n",
       "  1035483,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [180836,\n",
       "  360087,\n",
       "  737175,\n",
       "  923907,\n",
       "  364884,\n",
       "  26479,\n",
       "  1144446,\n",
       "  512957,\n",
       "  249430,\n",
       "  866075,\n",
       "  300748,\n",
       "  656421,\n",
       "  237767,\n",
       "  737175,\n",
       "  453360,\n",
       "  19465,\n",
       "  636225,\n",
       "  349661,\n",
       "  882790,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1145775,\n",
       "  180836,\n",
       "  360087,\n",
       "  180836,\n",
       "  360087,\n",
       "  180836,\n",
       "  360087,\n",
       "  673889,\n",
       "  457358,\n",
       "  737175,\n",
       "  923907,\n",
       "  377818,\n",
       "  277834,\n",
       "  360087,\n",
       "  568307,\n",
       "  530354,\n",
       "  28326,\n",
       "  512957,\n",
       "  530354,\n",
       "  457358,\n",
       "  896883,\n",
       "  971496,\n",
       "  343410,\n",
       "  19108,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [180836,\n",
       "  360087,\n",
       "  79695,\n",
       "  360087,\n",
       "  525904,\n",
       "  512957,\n",
       "  249430,\n",
       "  866075,\n",
       "  667639,\n",
       "  946624,\n",
       "  737175,\n",
       "  923907,\n",
       "  648441,\n",
       "  83236,\n",
       "  268756,\n",
       "  360087,\n",
       "  544864,\n",
       "  36803,\n",
       "  716832,\n",
       "  760362,\n",
       "  123265,\n",
       "  165606,\n",
       "  149864,\n",
       "  157742,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [581620,\n",
       "  845122,\n",
       "  965333,\n",
       "  760240,\n",
       "  382462,\n",
       "  946624,\n",
       "  685358,\n",
       "  180836,\n",
       "  360087,\n",
       "  243855,\n",
       "  349661,\n",
       "  1033289,\n",
       "  430748,\n",
       "  349661,\n",
       "  181110,\n",
       "  379085,\n",
       "  685358,\n",
       "  313446,\n",
       "  367632,\n",
       "  1046051,\n",
       "  1127956,\n",
       "  198131,\n",
       "  349661,\n",
       "  430748,\n",
       "  349661,\n",
       "  181110,\n",
       "  379085,\n",
       "  486395,\n",
       "  264778,\n",
       "  64627,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
