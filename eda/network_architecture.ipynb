{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SkipGramFoo(torch.nn.Module):\n",
    "#   def __init__(self, voc: int, emb: int, _):\n",
    "#     super().__init__()\n",
    "#     self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "#     self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "#     self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "#   def forward(self, inpt, trgs, rand):\n",
    "#     emb = self.emb(inpt)\n",
    "#     ctx = self.ffw.weight[trgs]\n",
    "#     rnd = self.ffw.weight[rand]\n",
    "#     out = torch.bmm(ctx, emb.unsqueeze(-1)).squeeze()\n",
    "#     rnd = torch.bmm(rnd, emb.unsqueeze(-1)).squeeze()\n",
    "#     out = self.sig(out)\n",
    "#     rnd = self.sig(rnd)\n",
    "#     pst = -out.log().mean()\n",
    "#     ngt = -(1 - rnd + 10**(-3)).log().mean()\n",
    "#     return pst + ngt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = (len(words_to_ids), 64, 2)\n",
    "# mFoo = SkipGramFoo(*args)\n",
    "# print('mFoo', sum(p.numel() for p in mFoo.parameters()))\n",
    "# opFoo = torch.optim.Adam(mFoo.parameters(), lr=0.003)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchinfo\n",
    "# from torchinfo import summary\n",
    "\n",
    "# mFoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows = list(more_itertools.windowed(tokens, 3))\n",
    "# inputs = [w[1] for w in windows]\n",
    "# targets = [[w[0], w[2]] for w in windows]\n",
    "# input_tensor = torch.LongTensor(inputs)\n",
    "# target_tensor = torch.LongTensor(targets)\n",
    "# dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project='mlx6-word2vec', name='mFoo')\n",
    "# mFoo.to(device)\n",
    "# for epoch in range(2):\n",
    "#   prgs = tqdm.tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "#   for inpt, trgs in prgs:\n",
    "#     inpt, trgs = inpt.to(device), trgs.to(device)\n",
    "#     rand = torch.randint(0, len(words_to_ids), (inpt.size(0), 2)).to(device)\n",
    "#     opFoo.zero_grad()\n",
    "#     loss = mFoo(inpt, trgs, rand)\n",
    "#     loss.backward()\n",
    "#     opFoo.step()\n",
    "#     wandb.log({'loss': loss.item()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the embedding matrix\n",
    "import joblib\n",
    "embedding_dict = joblib.load('embeds.pkl')\n",
    "\n",
    "word_to_id = joblib.load('word_to_ids.pkl')\n",
    "id_to_word = joblib.load('idx_to_word.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7k/nmbh9fxj4dsbx6d3jx2k1lvm0000gn/T/ipykernel_52265/2386894407.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  embeddings_tensor = torch.tensor([i for i in embedding_dict.values()], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# embed_dim = 300\n",
    "# embeddings_tensor = torch.zeros(len(word_to_id), embed_dim, dtype=torch.float32)  # or load your actual weights\n",
    "embeddings_tensor = torch.tensor([i for i in embedding_dict.values()], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1244025, 300])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocTower(nn.Module):\n",
    "    def __init__(self, embed_dim, sequence_length, lstm_hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        :param vocab_size: Size of the vocabulary.\n",
    "        :param embed_dim: Dimensionality of token embeddings.\n",
    "        :param lstm_hidden_dim: Number of hidden units in the LSTM.\n",
    "        :param output_dim: Final embedding dimension (common space).\n",
    "        \"\"\"\n",
    "        super(DocTower, self).__init__()\n",
    "        # Maps token indices to embeddings (each of size embed_dim), embed with a given embedding dictionary\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor, padding_idx=0, freeze=False)\n",
    "        # LSTM that processes sequences of embeddings.\n",
    "        # With batch_first=True, input shape: (batch_size, sequence_length, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, batch_first=True)\n",
    "        # Final projection to the common embedding space\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Tensor of shape (batch_size, sequence_length) containing token indices.\n",
    "        :return: Tensor of shape (batch_size, output_dim) representing the sentence embedding.\n",
    "        \"\"\"\n",
    "        # Convert token indices to embeddings.\n",
    "        # Output shape: (batch_size, sequence_length, embed_dim)\n",
    "        x = self.embedding(x)\n",
    "        print(x.shape)\n",
    "        # The LSTM processes the sequence in order (along the sequence_length dimension)\n",
    "        # 'output' has shape (batch_size, sequence_length, lstm_hidden_dim)\n",
    "        # 'hn' (hidden state) has shape (num_layers, batch_size, lstm_hidden_dim)\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        print(output.shape)\n",
    "        # Use the final hidden state from the last LSTM layer.\n",
    "        # This represents the entire sentence after processing all tokens.\n",
    "        last_hidden = hn[-1]  # Shape: (batch_size, lstm_hidden_dim)\n",
    "        \n",
    "        # Project the LSTM output into the common embedding space.\n",
    "        return self.fc(last_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, query_sequence_length, doc_sequence_length, output_dim, embed_dim, lstm_hidden_dim):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        self.query_tower = DocTower(embed_dim, query_sequence_length, lstm_hidden_dim, output_dim)\n",
    "        self.doc_tower = DocTower(embed_dim, doc_sequence_length, lstm_hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, query, doc_positive, doc_negative):\n",
    "        \"\"\"\n",
    "        :param query: Tensor of shape (batch_size, query_input_dim)\n",
    "        :param positive_context: Tensor of shape (batch_size, seq_length) containing token indices\n",
    "        :param candidate: Tensor of shape (batch_size, query_input_dim) for negative samples\n",
    "        \"\"\"\n",
    "        query_embed = self.query_tower(query)\n",
    "        positive_embed = self.doc_tower(doc_positive)\n",
    "        negative_embed = self.doc_tower(doc_negative)\n",
    "        return query_embed, positive_embed, negative_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data for the query tower work on the query triplets\n",
    "import json\n",
    "with open('/Users/yuliagoryachev/Documents/mlx/mlx_week2/two-tower-search/data/train_triples_sample.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('/Users/yuliagoryachev/Documents/mlx/mlx_week2/two-tower-search/eda/train_triples_v1.1.json') as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "with open('/Users/yuliagoryachev/Documents/mlx/mlx_week2/two-tower-search/eda/valid_triples_v1.1.json') as f:\n",
    "    val = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "42.8\n"
     ]
    }
   ],
   "source": [
    "average_sentence_length = sum(len(item['query'].split()) for item in data) / len(data)\n",
    "print(average_sentence_length)#this before stemming and removing stopwords\n",
    "average_context_length = sum(len(item['positive'].split()) for item in data) / len(data)\n",
    "print(average_context_length)#this before stemming and removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuliagoryachev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENT_LEN = 12\n",
    "CONTEXT_LEN = 50\n",
    "from nltk.stem import PorterStemmer\n",
    "import constants\n",
    "\n",
    "punctuation_map = constants.punctuation_map\n",
    "\n",
    "def pad_truncate(data, max_len):\n",
    "    res = []\n",
    "    for d in tqdm(data):\n",
    "        clean_sent = tokenize(d, punctuation_map=punctuation_map, stemmer=PorterStemmer(), junk_punctuations=True)\n",
    "        tokens = clean_sent[:max_len]\n",
    "        tokens += ['<unk>'] * (max_len - len(tokens))\n",
    "        res.append(tokens)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_query = pad_truncate(data, 'query', SENT_LEN)\n",
    "# padded_positive = pad_truncate(data, 'positive', CONTEXT_LEN)\n",
    "# padded_negative = pad_truncate(data, 'negative', CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to indices using the word_to_id dictionary\n",
    "import joblib\n",
    "word_to_id = joblib.load('/Users/yuliagoryachev/Documents/mlx/mlx_week2/two-tower-search/eda/word_to_ids.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to indices\n",
    "def convert_to_indices(data, word_to_id):\n",
    "    res = []\n",
    "    for d in tqdm(data):\n",
    "        res.append([word_to_id.get(w, word_to_id['<unk>']) for w in d])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_query = convert_to_indices(padded_query, word_to_id)\n",
    "# padded_positive = convert_to_indices(padded_positive, word_to_id)\n",
    "# padded_negative = convert_to_indices(padded_negative, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 22272.73it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 3170.93it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 3476.75it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 6125.59it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 2338.84it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 19377.42it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 32170.30it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 3064.76it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 3487.43it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 853802.34it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 77988.22it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 78780.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#Wrap in dataset and dataloader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, data, SENT_LEN, CONTEXT_LEN):\n",
    "        self.queries = [d['query'] for d in data]\n",
    "        self.positives = [d['positive'] for d in data]\n",
    "        self.negatives = [d['negative'] for d in data]\n",
    "        #pad truncates the data\n",
    "        self.queries = pad_truncate(self.queries, SENT_LEN)\n",
    "        self.positives = pad_truncate(self.positives, CONTEXT_LEN)\n",
    "        self.negatives = pad_truncate(self.negatives, CONTEXT_LEN)\n",
    "        #convert to indices\n",
    "        self.queries = convert_to_indices(self.queries, word_to_id)\n",
    "        self.positives = convert_to_indices(self.positives, word_to_id)\n",
    "        self.negatives = convert_to_indices(self.negatives, word_to_id)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.queries[idx], dtype=torch.long), \\\n",
    "                torch.tensor(self.positives[idx], dtype=torch.long), \\\n",
    "                torch.tensor(self.negatives[idx], dtype=torch.long)\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        queries, positives, negatives = zip(*batch)\n",
    "        queries = torch.stack(queries)\n",
    "        positives = torch.stack(positives)\n",
    "        negatives = torch.stack(negatives)\n",
    "        return queries, positives, negatives\n",
    "    \n",
    "\n",
    "# Wrap in dataset and dataloader\n",
    "train_dataset = QueryDataset(train[:320], SENT_LEN, CONTEXT_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=QueryDataset.collate_fn)\n",
    "\n",
    "val_dataset = QueryDataset(val[:320], SENT_LEN, CONTEXT_LEN)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=QueryDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 406126,  393570,  681503,  456929,  286699,  354713,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [  27549,  925304,  301028,  483917,  896005,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 426508,  426150,  312891,       0,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [  27549,  925304,  301028,  483917,  896005,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 426508,  426150,  312891,       0,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [1139912,  537817,       0,       0,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 686966,  798043,       0,       0,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 876576, 1218058,  948181,  655412,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [1142456,  720183,  925304,  883102,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [  27549,  883102, 1059660,  202540,  624393, 1214990,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 327053,  426508, 1107134, 1182124,  566207, 1011940,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [  27549,  973922,  383138, 1126286,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [  27524,       0,       0,       0,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 313154, 1108109,  691361,  424219,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 486193,       0,       0,       0,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [   6824, 1108570,  672335,  293605,  425836,  672335,  293605,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 313154, 1108109,  691361,  424219,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 807348,  657443, 1017037,       0,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 740252, 1176051, 1185095,   29645,  860445,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 559056,  644429,   70147,       0,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [  27549,  905018, 1168557,  274870,  574164,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [  27549,  905018, 1168557,  274870,  574164,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [  50122,       0,       0,       0,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 440620,  367060, 1083566,  401684, 1156375,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [  27549,  883102, 1059660,  202540,  624393, 1214990,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 876576, 1218058,  948181,  655412,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [  27549,  905018, 1168557,  274870,  574164,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [  27524,       0,       0,       0,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [1021744,  134460,  810390,  535474,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 128943,   20589,  739686, 1017037,   33902,  309358, 1228717,   57860,\n",
      "               0,       0,       0,       0],\n",
      "        [1139912,  537817,       0,       0,       0,       0,       0,       0,\n",
      "               0,       0,       0,       0],\n",
      "        [ 440620,  367060, 1083566,  401684, 1156375,       0,       0,       0,\n",
      "               0,       0,       0,       0]]), tensor([[ 997673,  733772,  710459,  ...,       0,       0,       0],\n",
      "        [ 301028,  586796,  462668,  ...,  826582,  462668,  712409],\n",
      "        [1081405,  697221,  828574,  ...,       0,       0,       0],\n",
      "        ...,\n",
      "        [ 641609, 1116283,   33902,  ...,       0,       0,       0],\n",
      "        [1139912,  537817,  456874,  ...,       0,       0,       0],\n",
      "        [ 440620,  367060,  346201,  ...,       0,       0,       0]]), tensor([[ 393570,  790422,  182125,  ...,       0,       0,       0],\n",
      "        [ 403886,  114965,   84690,  ...,  286330,  286330,       0],\n",
      "        [ 975647,  194864,  485876,  ..., 1144711,  883102,  278124],\n",
      "        ...,\n",
      "        [ 518043,  523987,  550540,  ...,       0,       0,       0],\n",
      "        [ 382949,  846233,  372780,  ...,       0,       0,       0],\n",
      "        [1224428, 1039336,  510952,  ...,       0,       0,       0]]))\n",
      "torch.Size([32, 12])\n",
      "torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    keep_one = i\n",
    "    print(i)\n",
    "    print(i[0].shape)\n",
    "    print(i[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query\n",
      "========\n",
      "rememb\n",
      "name\n",
      "author\n",
      "wrote\n",
      "cat\n",
      "hat\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "The positive\n",
      "========\n",
      "mani\n",
      "children\n",
      "adult\n",
      "enjoy\n",
      "book\n",
      "dr\n",
      "seuss\n",
      "actual\n",
      "know\n",
      "anyth\n",
      "dr\n",
      "seuss\n",
      "dr\n",
      "seuss\n",
      "born\n",
      "given\n",
      "name\n",
      "theodor\n",
      "seuss\n",
      "geisel\n",
      "grew\n",
      "springfield\n",
      "massachusett\n",
      "left\n",
      "town\n",
      "young\n",
      "man\n",
      "attend\n",
      "dartmouth\n",
      "colleg\n",
      "new\n",
      "hampshir\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "idx_to_word = joblib.load('/Users/yuliagoryachev/Documents/mlx/mlx_week2/two-tower-search/eda/idx_to_word.pkl')\n",
    "\n",
    "print('The query')\n",
    "print('========')\n",
    "for i in keep_one[0][0]:\n",
    "    #convert i from tensor to index\n",
    "    print(idx_to_word[i.item()])\n",
    "print('The positive')\n",
    "print('========')\n",
    "for i in keep_one[1][0]:\n",
    "    #convert i from tensor to index\n",
    "    print(idx_to_word[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1244025"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet loss: 0.9965289235115051\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters (example values)\n",
    "query_sequence_length = SENT_LEN\n",
    "doc_sequence_length = CONTEXT_LEN\n",
    "output_dim = 128\n",
    "embed_dim = 300\n",
    "lstm_hidden_dim = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Instantiate the model\n",
    "model = TwoTowerModel(query_sequence_length, doc_sequence_length, output_dim, embed_dim, lstm_hidden_dim)\n",
    "\n",
    "\n",
    "# Define triplet loss\n",
    "triplet_loss_fn = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, query, pos_doc, neg_doc):\n",
    "        # Normalize embeddings to compute cosine similarity\n",
    "        query_norm = query / query.norm(dim=1, keepdim=True)\n",
    "        pos_doc_norm = pos_doc / pos_doc.norm(dim=1, keepdim=True)\n",
    "        neg_doc_norm = neg_doc / neg_doc.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        pos_sim = torch.sum(query_norm * pos_doc_norm, dim=1) \n",
    "        neg_sim = torch.sum(query_norm * neg_doc_norm, dim=1)\n",
    "        \n",
    "        # Compute triplet loss\n",
    "        loss = torch.max(neg_sim - pos_sim + self.margin, torch.tensor(0)).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Dummy inputs:\n",
    "# Query and candidate inputs (feature vectors)\n",
    "#random integers tensor\n",
    "query = torch.randint(0, 10, (batch_size, query_sequence_length))\n",
    "doc_positive = torch.randint(0, 20, (batch_size, doc_sequence_length))\n",
    "doc_negative = torch.randint(0, 20, (batch_size, doc_sequence_length))\n",
    "\n",
    "# Forward pass\n",
    "query_embed, positive_embed, candidate_embed = model(query, doc_positive, doc_negative)\n",
    "query_embed = query_embed.float()\n",
    "positive_embed = positive_embed.float()\n",
    "candidate_embed = candidate_embed.float()\n",
    "\n",
    "# Compute loss (assuming query is the anchor, positive_embed is the positive, and candidate_embed is the negative)\n",
    "# loss = triplet_loss_fn(query_embed, positive_embed, candidate_embed)\n",
    "margin = 1.0\n",
    "loss = TripletLoss(margin)(query_embed, positive_embed, candidate_embed)\n",
    "print(f\"Triplet loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss: 1.9970744848251343\n",
      "Validation loss: 2.023116934299469\n",
      "Epoch 2, Batch 1, Loss: 1.7033318281173706\n",
      "Validation loss: 2.034234046936035\n",
      "Epoch 3, Batch 1, Loss: 0.8117779493331909\n",
      "Validation loss: 2.2215139269828796\n",
      "Epoch 4, Batch 1, Loss: 0.0247591994702816\n",
      "Validation loss: 2.2115682721138\n",
      "Epoch 5, Batch 1, Loss: 0.014092542231082916\n",
      "Validation loss: 2.2089731216430666\n",
      "Epoch 6, Batch 1, Loss: 0.005610410124063492\n",
      "Validation loss: 2.2389997243881226\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters (example values)\n",
    "query_sequence_length = SENT_LEN\n",
    "doc_sequence_length = CONTEXT_LEN\n",
    "output_dim = 128\n",
    "embed_dim = 300\n",
    "lstm_hidden_dim = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Instantiate the model\n",
    "model = TwoTowerModel(query_sequence_length, doc_sequence_length, output_dim, embed_dim, lstm_hidden_dim)\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, query, pos_doc, neg_doc):\n",
    "        # Normalize embeddings to compute cosine similarity\n",
    "        query_norm = query / query.norm(dim=1, keepdim=True)\n",
    "        pos_doc_norm = pos_doc / pos_doc.norm(dim=1, keepdim=True)\n",
    "        neg_doc_norm = neg_doc / neg_doc.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        pos_sim = torch.sum(query_norm * pos_doc_norm, dim=1) \n",
    "        neg_sim = torch.sum(query_norm * neg_doc_norm, dim=1)\n",
    "        \n",
    "        # Compute triplet loss\n",
    "        loss = torch.max(neg_sim - pos_sim + self.margin, torch.tensor(0)).mean()\n",
    "        return loss\n",
    "\n",
    "margin = 2.0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#iterate over batches of the data\n",
    "losses = []\n",
    "epochs = 6\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (query, positive, negative) in enumerate(train_loader):\n",
    "        query_embed, positive_embed, negative_embed = model(query, positive, negative)\n",
    "        query_embed = query_embed.float()\n",
    "        positive_embed = positive_embed.float()\n",
    "        negative_embed = negative_embed.float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = TripletLoss(margin)(query_embed, positive_embed, negative_embed)\n",
    "        loss.backward()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item()}\")\n",
    "    # Validation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():  \n",
    "        for query, positive, negative in val_loader:\n",
    "            query_embed, positive_embed, negative_embed = model(query, positive, negative)\n",
    "            query_embed = query_embed.float()\n",
    "            positive_embed = positive_embed.float()\n",
    "            negative_embed = negative_embed.float()\n",
    "            loss = TripletLoss(margin)(query_embed, positive_embed, negative_embed)\n",
    "            valid_loss += loss.item()\n",
    "    print(f\"Validation loss: {valid_loss / len(val_loader)}\")\n",
    "    # print(f\"Validation loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2216.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6955.73it/s]\n"
     ]
    }
   ],
   "source": [
    "#embed a query\n",
    "query = \"How to make a cake\"\n",
    "#pad and truncate\n",
    "query = pad_truncate([query], 4)\n",
    "query = convert_to_indices(query, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is rba\n",
      "rba\n",
      "722428\n",
      "========\n",
      "Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\n",
      "sinc\n",
      "401098\n",
      "rba\n",
      "722428\n",
      "outstand\n",
      "426933\n",
      "reput\n",
      "307411\n",
      "affect\n",
      "1188611\n",
      "secur\n",
      "734919\n",
      "npa\n",
      "740575\n",
      "scandal\n",
      "1018751\n",
      "rba\n",
      "722428\n",
      "subsidiari\n",
      "1036929\n",
      "involv\n",
      "70290\n",
      "bribe\n",
      "1027549\n",
      "oversea\n",
      "1098502\n",
      "offici\n",
      "888271\n",
      "australia\n",
      "187620\n",
      "might\n",
      "396564\n",
      "win\n",
      "276869\n",
      "lucr\n",
      "541905\n",
      "note\n",
      "287077\n",
      "print\n",
      "514386\n",
      "contract\n",
      "355724\n",
      "asset\n",
      "219015\n",
      "bank\n",
      "901419\n",
      "includ\n",
      "266098\n",
      "gold\n",
      "47891\n",
      "foreign\n",
      "572029\n",
      "exchang\n",
      "309358\n",
      "reserv\n",
      "1200897\n",
      "australia\n",
      "187620\n",
      "estim\n",
      "975472\n",
      "net\n",
      "1190646\n",
      "worth\n",
      "1157870\n",
      "billion\n",
      "1036595\n",
      "nearli\n",
      "21564\n",
      "rba\n",
      "722428\n",
      "employe\n",
      "573446\n",
      "work\n",
      "1116892\n",
      "headquart\n",
      "295060\n",
      "sydney\n",
      "301028\n",
      "new\n",
      "99397\n",
      "south\n",
      "820336\n",
      "wale\n",
      "330625\n",
      "busi\n",
      "322716\n",
      "resumpt\n",
      "760700\n",
      "site\n",
      "1116283\n",
      "========\n",
      "The cost of sales, also referred to as the cost of goods sold, is a measure of how much it costs a company to sell its products. The cost of sales is also a necessary step when a business is trying to determine the amount of gross profit made in a given period. Not every company calculates cost of sales the same way.\n",
      "cost\n",
      "426508\n",
      "sale\n",
      "345464\n",
      "also\n",
      "1004990\n",
      "refer\n",
      "1198195\n",
      "cost\n",
      "426508\n",
      "good\n",
      "1087220\n",
      "sold\n",
      "630601\n",
      "measur\n",
      "202232\n",
      "much\n",
      "1005008\n",
      "cost\n",
      "426508\n",
      "compani\n",
      "804678\n",
      "sell\n",
      "820451\n",
      "product\n",
      "1240402\n",
      "cost\n",
      "426508\n",
      "sale\n",
      "345464\n",
      "also\n",
      "1004990\n",
      "necessari\n",
      "644547\n",
      "step\n",
      "308841\n",
      "busi\n",
      "322716\n",
      "tri\n",
      "75500\n",
      "determin\n",
      "778512\n",
      "amount\n",
      "135454\n",
      "gross\n",
      "612363\n",
      "profit\n",
      "1234531\n",
      "made\n",
      "1185095\n",
      "given\n",
      "756967\n",
      "period\n",
      "382350\n",
      "everi\n",
      "202556\n",
      "compani\n",
      "804678\n",
      "calcul\n",
      "205678\n",
      "cost\n",
      "426508\n",
      "sale\n",
      "345464\n",
      "way\n",
      "337542\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "query = train[0]['query']\n",
    "print(query)\n",
    "query_embed = np.zeros((1, 300))\n",
    "count_q = 0\n",
    "for i in tokenize(query, punctuation_map=punctuation_map, stemmer=PorterStemmer(), junk_punctuations=True):\n",
    "    print(i)\n",
    "    token = word_to_id.get(i, word_to_id['<unk>'])\n",
    "    query_embed+=embedding_dict.get(token, np.zeros(300))\n",
    "    print(token)\n",
    "\n",
    "query_embed = query_embed/max(count_q,1)\n",
    "\n",
    "print('========')\n",
    "#embed a positive context\n",
    "positive_embed = np.zeros((1, 300))\n",
    "count_p = 0\n",
    "positive = train[0]['positive']\n",
    "print(positive)\n",
    "for i in tokenize(positive, punctuation_map=punctuation_map, stemmer=PorterStemmer(), junk_punctuations=True):\n",
    "    print(i)\n",
    "    token = word_to_id.get(i, word_to_id['<unk>'])\n",
    "    positive_embed+=embedding_dict.get(token, np.zeros(300))\n",
    "    print(token)\n",
    "\n",
    "positive_embed = positive_embed/max(count_p,1)\n",
    "\n",
    "print('========')\n",
    "#embed a negative context\n",
    "negative_embed = np.zeros((1, 300))\n",
    "count_p = 0\n",
    "negative = train[0]['negative']\n",
    "print(negative)\n",
    "for i in tokenize(negative, punctuation_map=punctuation_map, stemmer=PorterStemmer(), junk_punctuations=True):\n",
    "    print(i)\n",
    "    token = word_to_id.get(i, word_to_id['<unk>'])\n",
    "    negative_embed+=embedding_dict.get(token, np.zeros(300))\n",
    "    print(token)\n",
    "\n",
    "negative_embed = negative_embed/max(count_p,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47809076]]\n",
      "[[0.08634104]]\n"
     ]
    }
   ],
   "source": [
    "#cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(query_embed, positive_embed))\n",
    "print(cosine_similarity(query_embed, negative_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47809076495182673"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(query_embed * positive_embed).sum()/np.linalg.norm(query_embed)/np.linalg.norm(positive_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, validation = train_test_split(train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 473335/473335 [00:14<00:00, 33094.86it/s]\n",
      "100%|██████████| 473335/473335 [02:22<00:00, 3310.88it/s]\n",
      "100%|██████████| 473335/473335 [02:22<00:00, 3327.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average pooling\n",
      "tokenize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202858/202858 [00:05<00:00, 36655.28it/s]\n",
      "100%|██████████| 202858/202858 [01:03<00:00, 3183.26it/s]\n",
      "100%|██████████| 202858/202858 [01:01<00:00, 3322.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average pooling\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.queries = [d['query'] for d in data]\n",
    "        self.positives = [d['positive'] for d in data]\n",
    "        self.negatives = [d['negative'] for d in data]\n",
    "        print('tokenize')\n",
    "        self.queries = [tokenize(i,punctuation_map=punctuation_map, stemmer=PorterStemmer(), junk_punctuations=True)  for i in tqdm(self.queries)]\n",
    "        self.positives = [tokenize(i,punctuation_map=punctuation_map, stemmer=PorterStemmer(), junk_punctuations=True)  for i in tqdm(self.positives)]\n",
    "        self.negatives = [tokenize(i,punctuation_map=punctuation_map, stemmer=PorterStemmer(), junk_punctuations=True)  for i in tqdm(self.negatives)]\n",
    "\n",
    "        print('average pooling')\n",
    "        self.queries = [self.average_pooling(i) for i in tqdm(self.queries)]\n",
    "        self.positives = [self.average_pooling(i) for i in tqdm(self.positives)]\n",
    "        self.negatives = [self.average_pooling(i) for i in tqdm(self.negatives)]\n",
    "    \n",
    "    def average_pooling(self, data: List[str]):\n",
    "        embed = np.zeros((1, 300))\n",
    "        count = 0\n",
    "        for d in data:\n",
    "            token = word_to_id.get(d, word_to_id['<unk>'])\n",
    "            embed+=embedding_dict.get(token, np.zeros(300))\n",
    "            count+=1\n",
    "        # print(data,embed)\n",
    "        return embed/max(count, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.queries[idx], dtype=torch.float32), \\\n",
    "                torch.tensor(self.positives[idx], dtype=torch.float32), \\\n",
    "                torch.tensor(self.negatives[idx], dtype=torch.float32)\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        queries, positives, negatives = zip(*batch)\n",
    "        queries = torch.stack(queries)\n",
    "        positives = torch.stack(positives)\n",
    "        negatives = torch.stack(negatives)\n",
    "        return queries, positives, negatives\n",
    "    \n",
    "\n",
    "# Wrap in dataset and dataloader\n",
    "train_dataset = QueryDataset(train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=QueryDataset.collate_fn)\n",
    "\n",
    "val_dataset = QueryDataset(validation)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=QueryDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473335"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_dataset:\n",
    "#     keep_one = i\n",
    "#     print(i)\n",
    "#     print(i[0].shape)\n",
    "#     print(i[1].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'data'))\n",
    "# if src_path not in sys.path:\n",
    "#     sys.path.insert(0, src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneTower(nn.Module):\n",
    "    def __init__(self, embed_dim, med_dim, output_dim):\n",
    "        super(OneTower, self).__init__()\n",
    "        self.fc = nn.Linear(embed_dim, med_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(med_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, embed_dim, med_dim, output_dim):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        self.query_tower = OneTower(embed_dim, med_dim, output_dim)\n",
    "        self.doc_tower = OneTower(embed_dim, med_dim, output_dim)\n",
    "        \n",
    "    def forward(self, query, doc_positive, doc_negative):\n",
    "        \"\"\"\n",
    "        :param query: Tensor of shape (batch_size, query_input_dim)\n",
    "        :param positive_context: Tensor of shape (batch_size, seq_length) containing token indices\n",
    "        :param candidate: Tensor of shape (batch_size, query_input_dim) for negative samples\n",
    "        \"\"\"\n",
    "        query_embed = self.query_tower(query)\n",
    "        positive_embed = self.doc_tower(doc_positive)\n",
    "        negative_embed = self.doc_tower(doc_negative)\n",
    "        return query_embed, positive_embed, negative_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 6340, Loss: 0.072265625\n",
      "Validation loss: 0.0713960455665923\n",
      "Epoch 2, Batch 6340, Loss: 0.05037499591708183\n",
      "Validation loss: 0.05330694031839961\n",
      "Epoch 3, Batch 6340, Loss: 0.03793749213218689\n",
      "Validation loss: 0.03374140936945938\n",
      "Epoch 4, Batch 6340, Loss: 0.04115623980760574\n",
      "Validation loss: 0.03158577757120744\n",
      "Epoch 5, Batch 6340, Loss: 0.08165625482797623\n",
      "Validation loss: 0.051009222822504745\n",
      "Epoch 6, Batch 6340, Loss: 0.07856250554323196\n",
      "Validation loss: 0.04137919181340877\n",
      "Epoch 7, Batch 6340, Loss: 0.05351562425494194\n",
      "Validation loss: 0.04645971471578917\n",
      "Epoch 8, Batch 6340, Loss: 0.05351562425494194\n",
      "Validation loss: 0.05100312932785189\n",
      "Epoch 9, Batch 6340, Loss: 0.03804686665534973\n",
      "Validation loss: 0.0334361846347219\n",
      "Epoch 10, Batch 6340, Loss: 0.031749993562698364\n",
      "Validation loss: 0.026691635633005\n",
      "Epoch 11, Batch 6340, Loss: 0.013093739748001099\n",
      "Validation loss: 0.014776367540045257\n",
      "Epoch 12, Batch 6340, Loss: 0.028656240552663803\n",
      "Validation loss: 0.02092402807410442\n",
      "Epoch 13, Batch 6340, Loss: 0.02554686740040779\n",
      "Validation loss: 0.01990375506958986\n",
      "Epoch 14, Batch 6340, Loss: 0.01618748903274536\n",
      "Validation loss: 0.016003463151262432\n",
      "Epoch 15, Batch 6340, Loss: 0.01931249164044857\n",
      "Validation loss: 0.0220191895026032\n",
      "Epoch 16, Batch 6340, Loss: 0.016156241297721863\n",
      "Validation loss: 0.02427082957497178\n",
      "Epoch 17, Batch 6340, Loss: 0.01932811364531517\n",
      "Validation loss: 0.01987027313827599\n",
      "Epoch 18, Batch 6340, Loss: 0.013109365478157997\n",
      "Validation loss: 0.014211562417347678\n",
      "Epoch 19, Batch 6340, Loss: 0.01931249164044857\n",
      "Validation loss: 0.02181620740710702\n",
      "Epoch 20, Batch 6340, Loss: 0.025531243532896042\n",
      "Validation loss: 0.02414228621435466\n",
      "Epoch 21, Batch 6340, Loss: 0.013031241483986378\n",
      "Validation loss: 0.019901061970123594\n",
      "Epoch 22, Batch 6340, Loss: 0.016171865165233612\n",
      "Validation loss: 0.017556191326856426\n",
      "Epoch 23, Batch 6340, Loss: 0.009999990463256836\n",
      "Validation loss: 0.010474863953224445\n",
      "Epoch 24, Batch 6340, Loss: 0.009999990463256836\n",
      "Validation loss: 0.010462666167480995\n",
      "Epoch 25, Batch 6340, Loss: 0.009999990463256836\n",
      "Validation loss: 0.010522189495750124\n",
      "Epoch 26, Batch 6340, Loss: 0.009999990463256836\n",
      "Validation loss: 0.010789608943190908\n",
      "Epoch 27, Batch 6340, Loss: 0.013124990276992321\n",
      "Validation loss: 0.011203060694749845\n",
      "Epoch 28, Batch 6340, Loss: 0.009999990463256836\n",
      "Validation loss: 0.01211069308071506\n",
      "Epoch 29, Batch 6340, Loss: 0.009984364733099937\n",
      "Validation loss: 0.013774579593894139\n",
      "Epoch 30, Batch 6340, Loss: 0.01621874049305916\n",
      "Validation loss: 0.018081597074549\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters (example values)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "output_dim = 64\n",
    "med_dim = 128\n",
    "embed_dim = 300\n",
    "batch_size = 32\n",
    "\n",
    "# Instantiate the model\n",
    "model = TwoTowerModel(embed_dim, med_dim,  output_dim)\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, query, pos_doc, neg_doc):\n",
    "        pos_sim = F.cosine_similarity(query, pos_doc)\n",
    "        neg_sim = F.cosine_similarity(query, neg_doc)\n",
    "        \n",
    "        # Compute triplet loss with margin\n",
    "        loss = torch.clamp(self.margin - pos_sim + neg_sim, min=0.0)\n",
    "        return loss.mean()\n",
    "\n",
    "margin = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#iterate over batches of the data\n",
    "losses = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (query, positive, negative) in enumerate(val_loader):\n",
    "        query_embed, positive_embed, negative_embed = model(query, positive, negative)\n",
    "        query_embed = query_embed.float()\n",
    "        positive_embed = positive_embed.float()\n",
    "        negative_embed = negative_embed.float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = TripletLoss(margin)(query_embed, positive_embed, negative_embed)\n",
    "        loss.backward()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # if i % 10 == 0:\n",
    "    print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item()}\")\n",
    "            \n",
    "    # Validation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():  \n",
    "        for query, positive, negative in val_loader:\n",
    "            query_embed, positive_embed, negative_embed = model(query, positive, negative)\n",
    "            query_embed = query_embed.float()\n",
    "            positive_embed = positive_embed.float()\n",
    "            negative_embed = negative_embed.float()\n",
    "            loss = TripletLoss(margin)(query_embed, positive_embed, negative_embed)\n",
    "            valid_loss += loss.item()\n",
    "    print(f\"Validation loss: {valid_loss / len(val_loader)}\")\n",
    "    # print(f\"Validation loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
