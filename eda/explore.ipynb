{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuliagoryachev/miniconda3/envs/mlx_course/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['validation', 'train', 'test'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': ['Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.'],\n",
       " 'passages': {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  'passage_text': [\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "   \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       "   'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ',\n",
       "   'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       "   'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       "   'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.',\n",
       "   'RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       "   'vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.',\n",
       "   'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       "   'Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. '],\n",
       "  'url': ['https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "   'https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "   'http://acronyms.thefreedictionary.com/RBA',\n",
       "   'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "   'http://rba-africa.com/about/what-is-rba/',\n",
       "   'http://resultsleadership.org/what-is-results-based-accountability-rba/',\n",
       "   'http://rba-africa.com/about/what-is-rba/',\n",
       "   'http://searchsecurity.techtarget.com/definition/risk-based-authentication-RBA',\n",
       "   'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "   'http://www.rbaconsulting.com/']},\n",
       " 'query': 'what is rba',\n",
       " 'query_id': 19699,\n",
       " 'query_type': 'description',\n",
       " 'wellFormedAnswers': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       " \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n",
       " 'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ',\n",
       " 'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       " 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       " 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.',\n",
       " 'RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;',\n",
       " 'vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.',\n",
       " 'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.',\n",
       " 'Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['passages']['passage_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82326"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82326 10047 9650\n"
     ]
    }
   ],
   "source": [
    "train = ds['train']\n",
    "valid = ds['validation']\n",
    "test = ds['test']\n",
    "print(len(train), len(valid), len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.history.com/topics/us-presidents/ronald-reagan',\n",
       " 'https://en.wikipedia.org/wiki/Reagan_Democrat',\n",
       " 'http://www.answers.com/Q/Was_Ronald_Reagan_a_republican_or_a_democrat',\n",
       " 'https://en.wikipedia.org/wiki/Ronald_Reagan',\n",
       " 'http://www.msnbc.com/the-last-word/watch/when-reagan-was-a-liberal-democrat-219696195576',\n",
       " 'http://www.history.com/topics/us-presidents/ronald-reagan',\n",
       " 'http://www.biography.com/people/ronald-reagan-9453198']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][1]['passages']['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triples = []\n",
    "for i in tqdm(range(0,len(train))):\n",
    "    query = ds['train'][i]['query']\n",
    "    for k, passage in enumerate(ds['train'][i]['passages']['passage_text']):\n",
    "        # print(passage)\n",
    "        sample = {}\n",
    "        sample['query'] = query\n",
    "        sample['positive'] = passage\n",
    "        sample['positive_url'] = ds['train'][i]['passages']['url'][k]\n",
    "        while True:\n",
    "            random_ind = random.randint(0, len(ds['train'])-1)\n",
    "            if random_ind != i:\n",
    "                break\n",
    "\n",
    "        negatives = ds['train'][random_ind]['passages']['passage_text']\n",
    "        #make random selection of these passages\n",
    "        sample['negative'] = random.choice(negatives)\n",
    "        sample['negative_url'] = ds['train'][random_ind]['passages']['url'][negatives.index(sample['negative'])]\n",
    "        train_triples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the train triples\n",
    "import json\n",
    "json.dump(train_triples, open('train_triples_v1.1.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(train_triples[:5], open('train_triples_sample.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10047/10047 [00:11<00:00, 899.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#validation triples\n",
    "valid_triples = []\n",
    "for i in tqdm(range(len(valid))):\n",
    "    query = ds['validation'][i]['query']\n",
    "    for k, passage in enumerate(ds['validation'][i]['passages']['passage_text']):\n",
    "        # print(passage)\n",
    "        sample = {}\n",
    "        sample['query'] = query\n",
    "        sample['positive'] = passage\n",
    "        sample['positive_url'] = ds['validation'][i]['passages']['url'][k]\n",
    "        while True:\n",
    "            random_ind = random.randint(0, len(ds['validation'])-1)\n",
    "            if random_ind != i:\n",
    "                break\n",
    "\n",
    "        negatives = ds['validation'][random_ind]['passages']['passage_text']\n",
    "        #make random selection of these passages\n",
    "        sample['negative'] = random.choice(negatives)\n",
    "        sample['negative_url'] = ds['validation'][random_ind]['passages']['url'][negatives.index(sample['negative'])]\n",
    "        valid_triples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(valid_triples, open('valid_triples_v1.1.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuliagoryachev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/yuliagoryachev/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['went', 'market', 'today', 'left', 'yet', 'found', 'dog', 'toy']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import constants\n",
    "\n",
    "punctuation_map = constants.punctuation_map\n",
    "tokenize(\"I went to the market today and I haven't left yet or found a dog's toy\", \n",
    "         punctuation_map=punctuation_map, stemmer=PorterStemmer(), junk_punctuations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop punctuation map to json\n",
    "import json\n",
    "json.dump(punctuation_map, open('punctuation_map.json', 'w'))\n",
    "\n",
    "#load punctuation map\n",
    "with open('punctuation_map.json', 'r') as f:\n",
    "    punctuation_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82326/82326 [00:04<00:00, 20569.45it/s]\n",
      "100%|██████████| 10047/10047 [00:00<00:00, 20681.78it/s]\n",
      "100%|██████████| 9650/9650 [00:00<00:00, 20687.78it/s]\n"
     ]
    }
   ],
   "source": [
    "queries_t = []\n",
    "for i in tqdm(range(0,len(train))):\n",
    "    queries_t.append(train[i]['query'])\n",
    "\n",
    "for i in tqdm(range(0,len(valid))):\n",
    "    queries_t.append(valid[i]['query'])\n",
    "\n",
    "for i in tqdm(range(0,len(test))):\n",
    "    queries_t.append(test[i]['query'])\n",
    "\n",
    "queries_t = list(set(queries_t))\n",
    "queries_t = ' '.join(queries_t)\n",
    "# queries_words = tokenize(queries_t, punctuation_map=punctuation_map, stemmer=PorterStemmer(), junk_punctuations=True)\n",
    "queries_words = word_tokenize(queries_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11609260833036346"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "oov = 0\n",
    "for v in queries_words:\n",
    "    if not v in model:\n",
    "        oov+=1\n",
    "\n",
    "oov/len(queries_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82326/82326 [02:01<00:00, 676.57it/s]\n",
      "100%|██████████| 10047/10047 [00:14<00:00, 682.47it/s]\n",
      "100%|██████████| 9650/9650 [00:14<00:00, 673.38it/s]\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for i in tqdm(range(0,len(train))):\n",
    "    zz = train[i]['passages']['passage_text']\n",
    "    zz = ' '.join(zz)\n",
    "    queries_words = word_tokenize(zz)\n",
    "    words.extend(list(set(queries_words)))\n",
    "\n",
    "for i in tqdm(range(0,len(valid))):\n",
    "    zz = valid[i]['passages']['passage_text']\n",
    "    zz = ' '.join(zz)\n",
    "    queries_words = word_tokenize(zz)\n",
    "    words.extend(list(set(queries_words)))\n",
    "\n",
    "for i in tqdm(range(0,len(test))):\n",
    "    zz = valid[i]['passages']['passage_text']\n",
    "    zz = ' '.join(zz)\n",
    "    queries_words = word_tokenize(zz)\n",
    "    words.extend(list(set(queries_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12011768627773362"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "oov = 0\n",
    "for v in words:\n",
    "    if not v in model:\n",
    "        oov+=1\n",
    "\n",
    "oov/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641954 317\n"
     ]
    }
   ],
   "source": [
    "print(len(set(words)), len(set(queries_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(set(queries_words + words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6680478663580256\n"
     ]
    }
   ],
   "source": [
    "oov = 0\n",
    "for v in all_words:\n",
    "    if not v in model:\n",
    "        oov+=1\n",
    "\n",
    "print(oov/len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump all words to a file\n",
    "import json\n",
    "json.dump(list(set(queries_words)), open('queries_words.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118621"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(queries_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808731/808731 [07:13<00:00, 1864.18it/s]\n",
      "100%|██████████| 101093/101093 [03:56<00:00, 426.86it/s]\n",
      "100%|██████████| 101092/101092 [00:52<00:00, 1921.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# passages_words = []\n",
    "# for i in tqdm(range(0,len(train))):\n",
    "#     passages = ' '.join((train[i]['passages']['passage_text']))\n",
    "#     w = tokenize(passages, {})\n",
    "#     passages_words.extend(w)\n",
    "# #dump all words to a file\n",
    "# json.dump(list(set(passages_words)), open('passages_words.json', 'w'))\n",
    "# for i in tqdm(range(0,len(valid))):\n",
    "#     passages = ' '.join((train[i]['passages']['passage_text']))\n",
    "#     w = tokenize(passages, punctuation_map={}, stemmer=PorterStemmer())\n",
    "#     passages_words.extend(w)\n",
    "# #dump all words to a file\n",
    "# json.dump(list(set(passages_words)), open('passages_words.json', 'w'))\n",
    "# for i in tqdm(range(0,len(test))):\n",
    "#     passages = ' '.join((train[i]['passages']['passage_text']))\n",
    "#     w = tokenize(passages, {})\n",
    "#     passages_words.extend(w)\n",
    "# #dump all words to a file\n",
    "# json.dump(list(set(passages_words)), open('passages_words.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322286825"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(passages_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "qwords = json.load(open('queries_words.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwords = json.load(open('passages_words.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1225032"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(set(qwords + pwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1244024\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ids = {v: i+1 for i,v in enumerate(all_words)}\n",
    "word_to_ids['<unk>'] = 0\n",
    "idx_to_word = {v: k for k, v in word_to_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeds = {}\n",
    "oov = 0\n",
    "for v, ind in word_to_ids.items():\n",
    "    if v in model:\n",
    "        embeds[ind] = model[v]\n",
    "    else:\n",
    "        oov += 1\n",
    "        embeds[ind] = np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8901565483008782"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov/len(word_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wce', 'nterjections', 'psychotoxic', 'polyphobia', 'gourka']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_ids.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idx_to_word.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save all\n",
    "import joblib\n",
    "\n",
    "# joblib.dump(embeds, 'embeds.pkl')\n",
    "joblib.dump(word_to_ids, 'word_to_ids.pkl')\n",
    "joblib.dump(idx_to_word, 'idx_to_word.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "word_to_ids = joblib.load('word_to_ids.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make vocab with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82326/82326 [00:04<00:00, 17014.34it/s]\n",
      "100%|██████████| 10047/10047 [00:00<00:00, 18028.68it/s]\n",
      "100%|██████████| 9650/9650 [00:00<00:00, 16323.89it/s]\n"
     ]
    }
   ],
   "source": [
    "queries_t = []\n",
    "for i in tqdm(range(0,len(train))):\n",
    "    queries_t.append(train[i]['query'])\n",
    "\n",
    "for i in tqdm(range(0,len(valid))):\n",
    "    queries_t.append(valid[i]['query'])\n",
    "\n",
    "for i in tqdm(range(0,len(test))):\n",
    "    queries_t.append(test[i]['query'])\n",
    "\n",
    "queries_t = list(set(queries_t))\n",
    "queries_t = ' '.join(queries_t)\n",
    "queries_words = word_tokenize(queries_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82326/82326 [03:11<00:00, 430.68it/s]\n",
      "100%|██████████| 10047/10047 [00:27<00:00, 371.00it/s]\n",
      "100%|██████████| 9650/9650 [00:23<00:00, 418.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "passages_words = []\n",
    "for i in tqdm(range(0,len(train))):\n",
    "    passages = ' '.join((train[i]['passages']['passage_text']))\n",
    "    w = word_tokenize(passages)\n",
    "    passages_words.extend(w)\n",
    "#dump all words to a file\n",
    "json.dump(list(set(passages_words)), open('passages_words.json', 'w'))\n",
    "for i in tqdm(range(0,len(valid))):\n",
    "    passages = ' '.join((train[i]['passages']['passage_text']))\n",
    "    w = word_tokenize(passages)\n",
    "    passages_words.extend(w)\n",
    "#dump all words to a file\n",
    "json.dump(list(set(passages_words)), open('passages_words.json', 'w'))\n",
    "for i in tqdm(range(0,len(test))):\n",
    "    passages = ' '.join((train[i]['passages']['passage_text']))\n",
    "    w = word_tokenize(passages)\n",
    "    passages_words.extend(w)\n",
    "#dump all words to a file\n",
    "json.dump(list(set(passages_words)), open('passages_words.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(set(queries_words + passages_words))\n",
    "word_to_ids = {v: i+1 for i,v in enumerate(all_words)}\n",
    "word_to_ids['<unk>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = {v: k for k, v in word_to_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idx_to_word.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(word_to_ids, 'word_to_ids.pkl')\n",
    "joblib.dump(idx_to_word, 'idx_to_word.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
